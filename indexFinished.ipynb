{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Productionizing a Model with Docker and SageMaker\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this codealong, you'll see an example of the steps needed in order to productionize your own model with AWS SageMaker. \n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "This notebook contains a lot of _boilerplate code_ provided by Amazon which you'll need to make use of pretty much anytime you need to productionize a model. Much of this can be copied and pasted over, although it's likely that some modifications will be needed on a project-by-project basis. Going forward, use this notebook as a reference to remember the necessary steps for productionizing a model with AWS SageMaker -- you are _not_ expected to remember or even understand most of the code in this notebook, as you aren't yet familiar with AWS and its quirks. That's okay -- notebooks like these are meant to help you through productionizing a model until you've done it a few times and start to understand the process and the necessary code needed to make it all work!\n",
    "\n",
    "This notebook borrows it's general structure, as well as all boilerplate code, from this [training repo](https://github.com/aws-samples/amazon-sagemaker-keras-text-classification) provided publicly by AWS. For more examples of how to use various AWS services, check out the [AWS-Samples Repository](https://github.com/aws-samples)!\n",
    "\n",
    "## Overview of Process\n",
    "\n",
    "When productionizing a machine learning model using AWS, you'll typically use the following workflow:\n",
    "\n",
    "1. Explore and preprocess data\n",
    "2. Build SageMaker container (Docker)\n",
    "3. Test training and inference code on your local machine \n",
    "4. Train and deploy model with SageMaker\n",
    "\n",
    "To help simplify this process, AWS has provided some example repos containing tutorials and boilerplate code that we will make use of to help us through this process. Any code found in this notebook is primarily concerned with Step 4, training and deploying the model to SageMaker. However, the code in this notebook will only work if you have worked through the sample labs provided by AWS for steps 1 - 3, which will help you gain familiarity working with the platform. You'll find the link to these sample labs below. \n",
    "\n",
    "## Step 0: Complete Lead-Up Notebooks from AWS Training Team\n",
    "\n",
    "**_IMPORTANT:_** Before beginning on this lab, you need to complete labs 1, 2, and 3 from this [AWS Training Repo](https://github.com/aws-samples/amazon-sagemaker-keras-text-classification). Read them thoroughly, and complete each step for each lab. These labs handle much of the basic setup needed in order to make sure the code in this notebook will work, so **_do not skip them!_**\n",
    "\n",
    "Once you have completed those labs, come back here and move on to Step 1 below. \n",
    "\n",
    "## Step 1: Building and Registering the Container\n",
    "\n",
    "If you are at the `container` folder within this repo, you'll see it contains some Docker images. These are docker images that you'll need to make sure you use in your own projects when productionizing a model on AWS. \n",
    "\n",
    "Everything in the cell below is boilerplate code that uses the `container` folder in order to create and register the docker image needed on AWS. \n",
    "\n",
    "At this point, it is best that you upload this notebook in your Jupyter's instance of SageMaker in order to run the following cells!\n",
    "\n",
    "After you have successfully uploaded this notebook, if you are asked to choose a kernel, use the same kernel which runs the `sagemaker_keras_text_classification.ipynb` notebook. \n",
    "\n",
    "> NOTE: If you deactivated this process (stopped your Jupyter instance, like _Step 8_ below) and then came back to continue, you'll need to go back and start from [Lab 2](https://github.com/aws-samples/amazon-sagemaker-keras-text-classification#lab-2-building-the-sagemaker-tensorflow-container), because you need the Docker instance running in order to run the following cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Stopping docker: [  OK  ]\n",
      "Starting docker:\t.[  OK  ]\n",
      "Sending build context to Docker daemon  456.3MB\n",
      "Step 1/6 : FROM 763104351884.dkr.ecr.us-east-2.amazonaws.com/tensorflow-training:1.14.0-cpu-py36-ubuntu16.04\n",
      " ---> e6a210ff54e4\n",
      "Step 2/6 : RUN apt-get update &&     apt-get install -y nginx\n",
      " ---> Using cache\n",
      " ---> 368aede281a8\n",
      "Step 3/6 : RUN pip install gevent gunicorn flask\n",
      " ---> Using cache\n",
      " ---> 903901c8fe78\n",
      "Step 4/6 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> f678fd80ca2c\n",
      "Step 5/6 : COPY sagemaker_keras_text_classification /opt/program\n",
      " ---> Using cache\n",
      " ---> 292fac0c72cd\n",
      "Step 6/6 : WORKDIR /opt/program\n",
      " ---> Using cache\n",
      " ---> 2e4324c1da32\n",
      "Successfully built 2e4324c1da32\n",
      "Successfully tagged sagemaker-keras-text-classification:latest\n",
      "The push refers to repository [784123579867.dkr.ecr.us-east-2.amazonaws.com/sagemaker-keras-text-classification]\n",
      "d5060237a486: Preparing\n",
      "e78bffdf5924: Preparing\n",
      "fafcc8b1d4b8: Preparing\n",
      "cc978a7bbd2a: Preparing\n",
      "3a97a8d562fb: Preparing\n",
      "cb460459ddc8: Preparing\n",
      "b4064660a4cf: Preparing\n",
      "b6e9883adafa: Preparing\n",
      "9ee6d909e5a7: Preparing\n",
      "e722e212cbab: Preparing\n",
      "708ade65e147: Preparing\n",
      "11fc4467b8a3: Preparing\n",
      "0cf88c3675cd: Preparing\n",
      "d456742927ee: Preparing\n",
      "8722c9641a57: Preparing\n",
      "7083756ef61f: Preparing\n",
      "9d2fda619715: Preparing\n",
      "e79142719515: Preparing\n",
      "aeda103e78c9: Preparing\n",
      "2558e637fbff: Preparing\n",
      "f749b9b0fb21: Preparing\n",
      "cb460459ddc8: Waiting\n",
      "b4064660a4cf: Waiting\n",
      "b6e9883adafa: Waiting\n",
      "9ee6d909e5a7: Waiting\n",
      "e722e212cbab: Waiting\n",
      "708ade65e147: Waiting\n",
      "11fc4467b8a3: Waiting\n",
      "0cf88c3675cd: Waiting\n",
      "d456742927ee: Waiting\n",
      "8722c9641a57: Waiting\n",
      "7083756ef61f: Waiting\n",
      "9d2fda619715: Waiting\n",
      "e79142719515: Waiting\n",
      "aeda103e78c9: Waiting\n",
      "2558e637fbff: Waiting\n",
      "f749b9b0fb21: Waiting\n",
      "e78bffdf5924: Layer already exists\n",
      "3a97a8d562fb: Layer already exists\n",
      "d5060237a486: Layer already exists\n",
      "cc978a7bbd2a: Layer already exists\n",
      "fafcc8b1d4b8: Layer already exists\n",
      "cb460459ddc8: Layer already exists\n",
      "b6e9883adafa: Layer already exists\n",
      "9ee6d909e5a7: Layer already exists\n",
      "e722e212cbab: Layer already exists\n",
      "b4064660a4cf: Layer already exists\n",
      "11fc4467b8a3: Layer already exists\n",
      "d456742927ee: Layer already exists\n",
      "0cf88c3675cd: Layer already exists\n",
      "8722c9641a57: Layer already exists\n",
      "708ade65e147: Layer already exists\n",
      "7083756ef61f: Layer already exists\n",
      "9d2fda619715: Layer already exists\n",
      "e79142719515: Layer already exists\n",
      "aeda103e78c9: Layer already exists\n",
      "2558e637fbff: Layer already exists\n",
      "f749b9b0fb21: Layer already exists\n",
      "latest: digest: sha256:f89a4a33d12fb30adb11a6410943906823bddf126a44cfec42d28617e48870ff size: 4711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=sagemaker-keras-text-classification\n",
    "\n",
    "cd container\n",
    "\n",
    "chmod +x sagemaker_keras_text_classification/train\n",
    "chmod +x sagemaker_keras_text_classification/serve\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-east-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "# On a SageMaker Notebook Instance, the docker daemon may need to be restarted in order\n",
    "# to detect your network configuration correctly.  (This is a known issue.)\n",
    "if [ -d \"/home/ec2-user/SageMaker\" ]; then\n",
    "  sudo service docker restart\n",
    "fi\n",
    "\n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setting Up the Environment\n",
    "\n",
    "Once we've created the container, we'll need to set up the environment. The cell below contains more boilerplate code, which is used to handle a couple sticking points in order to set up the environment. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 prefix\n",
    "prefix = 'sagemaker-keras-text-classification'\n",
    "\n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Creating the Session\n",
    "\n",
    "Now that we've created the container and set up our environment, the next step is to create a SageMaker session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sess = sage.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Upload the Data for Training\n",
    "\n",
    "Steps 4 and 5 are where you'll add the code unique to your project.In this step, make sure have a folder called `'data'` that contains the data you'll be working with. The actual structure of the data is up to you, as you'll be the one consuming it to train your model in step 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIRECTORY = 'data'\n",
    "\n",
    "data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Fitting the Model\n",
    "\n",
    "This is the part where you'll do the brunt of the work. You'll train your own model on the data you uploaded in the previous step. Note that in the sample code below, the first 3 lines are boilerplate code. The actual creation and training of the model happen on the last two lines of code, where `tree` is instantiated and used. \n",
    "\n",
    "**_NOTE_**: You may have noticed that the code in the cell below uses an `Estimator` from `sage` (which is just an alias we set for `sagemaker` up above), the SageMaker library for python, rather than a model from scikit-learn. The `sagemaker` library contains a massive amount of useful models that we can use directly. Under the hood, the `sagemaker` library wraps in the same open-source frameworks such as scikit-learn, Keras, and TensorFlow that you're used to using. The code below is an example from AWS of how to use one of their `Estimator` objects for training. If you read the output of the cell when you run everything, you'll notice that much of it is warning messages or other printouts from sklearn and keras!\n",
    "\n",
    "For more information on the models and other tools included in the aws sagemaker library, check out [Amazon SageMaker Python SDK Documentation](https://sagemaker.readthedocs.io/en/stable/)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 03:11:31 Starting - Starting the training job...\n",
      "2020-12-06 03:11:33 Starting - Launching requested ML instances......\n",
      "2020-12-06 03:12:38 Starting - Preparing the instances for training...\n",
      "2020-12-06 03:13:25 Downloading - Downloading input data...\n",
      "2020-12-06 03:13:42 Training - Downloading the training image.....\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34mStarting the training.\u001b[0m\n",
      "\u001b[34m                                               TITLE  ...      TIMESTAMP\u001b[0m\n",
      "\u001b[34m1  Fed official says weak data caused by weather,...  ...  1394470370698\u001b[0m\n",
      "\u001b[34m2  Fed's Charles Plosser sees high bar for change...  ...  1394470371207\u001b[0m\n",
      "\u001b[34m3  US open: Stocks fall after Fed official hints ...  ...  1394470371550\u001b[0m\n",
      "\u001b[34m4  Fed risks falling 'behind the curve', Charles ...  ...  1394470371793\u001b[0m\n",
      "\u001b[34m5  Fed's Plosser: Nasty Weather Has Curbed Job Gr...  ...  1394470372027\n",
      "\u001b[0m\n",
      "\u001b[34m[5 rows x 7 columns]\u001b[0m\n",
      "\n",
      "2020-12-06 03:14:35 Training - Training image download completed. Training in progress.\u001b[34mFound 65990 unique tokens.\u001b[0m\n",
      "\u001b[34mShape of data tensor: (422417, 100)\u001b[0m\n",
      "\u001b[34mShape of label tensor: (422417, 4)\u001b[0m\n",
      "\u001b[34mx_train shape:  (337933, 100)\u001b[0m\n",
      "\u001b[34mWARNING: Logging before flag parsing goes to stderr.\u001b[0m\n",
      "\u001b[34mW1206 03:15:07.667263 140222093289216 deprecation.py:506] From /usr/local/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mCall initializer instance with the dtype argument instead of passing it to the constructor\u001b[0m\n",
      "\u001b[34mW1206 03:15:07.694521 140222093289216 deprecation.py:506] From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mCall initializer instance with the dtype argument instead of passing it to the constructor\u001b[0m\n",
      "\u001b[34mModel: \"sequential\"\u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mLayer (type)                 Output Shape              Param #   \u001b[0m\n",
      "\u001b[34m=================================================================\u001b[0m\n",
      "\u001b[34membedding (Embedding)        (None, 100, 100)          1000000   \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mflatten (Flatten)            (None, 10000)             0         \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mdense (Dense)                (None, 2)                 20002     \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mdense_1 (Dense)              (None, 2)                 6         \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mdense_2 (Dense)              (None, 4)                 12        \u001b[0m\n",
      "\u001b[34m=================================================================\u001b[0m\n",
      "\u001b[34mTotal params: 1,020,020\u001b[0m\n",
      "\u001b[34mTrainable params: 1,020,020\u001b[0m\n",
      "\u001b[34mNon-trainable params: 0\u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34m2020-12-06 03:15:07.737265: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX512F\u001b[0m\n",
      "\u001b[34mTo enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2020-12-06 03:15:07.775188: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2999995000 Hz\u001b[0m\n",
      "\u001b[34m2020-12-06 03:15:07.775566: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3fb6120 executing computations on platform Host. Devices:\u001b[0m\n",
      "\u001b[34m2020-12-06 03:15:07.775587: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\u001b[0m\n",
      "\u001b[34m2020-12-06 03:15:07.776456: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\u001b[0m\n",
      "\u001b[34m2020-12-06 03:15:07.804731: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34mTrain on 337933 samples, validate on 84484 samples\u001b[0m\n",
      "\u001b[34mEpoch 1/2\u001b[0m\n",
      "\u001b[34m337933/337933 - 31s - loss: 0.6077 - acc: 0.7741 - val_loss: 0.5545 - val_acc: 0.7958\u001b[0m\n",
      "\u001b[34mEpoch 2/2\u001b[0m\n",
      "\u001b[34m337933/337933 - 31s - loss: 0.5444 - acc: 0.7988 - val_loss: 0.5437 - val_acc: 0.8001\u001b[0m\n",
      "\u001b[34mTraining complete. Now saving model to:  /opt/ml/model\u001b[0m\n",
      "\u001b[34mW1206 03:16:09.702962 140222093289216 deprecation.py:506] From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mCall initializer instance with the dtype argument instead of passing it to the constructor\u001b[0m\n",
      "\u001b[34mW1206 03:16:09.703336 140222093289216 deprecation.py:506] From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mCall initializer instance with the dtype argument instead of passing it to the constructor\u001b[0m\n",
      "\u001b[34mTest headline:  What Improved Tech Means for Electric, Self-Driving and Flying Cars\u001b[0m\n",
      "\u001b[34mPredicted category:  t\u001b[0m\n",
      "\n",
      "2020-12-06 03:16:18 Uploading - Uploading generated training model\n",
      "2020-12-06 03:16:18 Completed - Training job completed\n",
      "Training seconds: 173\n",
      "Billable seconds: 173\n"
     ]
    }
   ],
   "source": [
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/sagemaker-keras-text-classification'.format(account, region)\n",
    "\n",
    "tree = sage.estimator.Estimator(image,\n",
    "                       role, 1, 'ml.c5.2xlarge',\n",
    "                       output_path=\"s3://{}/output\".format(sess.default_bucket()),\n",
    "                       sagemaker_session=sess)\n",
    "\n",
    "tree.fit(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Deploying the Model\n",
    "\n",
    "This is where the magic happens -- we have a trained model, and now we need to actually **_deploy_** it to the AWS cloud! Notice how during this step, we include a `json_serializer` -- this is so that the model can serialize and deserialize data as needed when taking data in as input. \n",
    "\n",
    "Running the cell below will create an endpoint for your trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import json_serializer\n",
    "predictor = tree.deploy(1, 'ml.t2.medium', serializer=json_serializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Cleanup  (IMPORTANT!)\n",
    "\n",
    "As a final step for this exercise, be sure to run the following line of code to delete your endpoint! Although you are running this lab on the free tier, you don't want to leave it running, because that is how costs can accrue. Run the cell below to delete your endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sagemaker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0eaa4309fb53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# sess.delete_endpoint(predictor.endpoint)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sagemaker' is not defined"
     ]
    }
   ],
   "source": [
    "import sagemaker.pre\n",
    "# sess.delete_endpoint(predictor.endpoint)\n",
    "sagemaker.predictor.Predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Deactivate Everything in AWS\n",
    "\n",
    "In AWS, you pay for usage. This means that anything left running is being used.  While the AWS Free Tier we've signed up for allows us to do small things for free for prototyping or learning, leaving some things running may take us past the usage limits for the AWS Free Tier. In order to avoid getting charged, you'll need to do the following steps: \n",
    "\n",
    "### 8.1: Deactivate the notebook in Sagemaker\n",
    "\n",
    "First, you'll need to deactivate your notebook in SageMaker. When you enter the SageMaker platform, you'll always see the number of open notebooks you have up and running highlighted in green under the 'Recent Activity' section. \n",
    "\n",
    "<img src='images/create-notebook-7.png'>\n",
    "\n",
    "To deactivate a running notebook, select it and then go to the 'Actions' tab and select stop. Stopping the notebook instance will take a minute or two. You'll know it's done when you see the 'Status' column for the highlighted notebook change from 'InService' to 'Stopped'. \n",
    "\n",
    "<img src='images/create-notebook-8.png'>\n",
    "\n",
    "### 8.2: Keep an Eye on Cost Explorer\n",
    "\n",
    "As you've seen from this lab, getting a handle on all the different services in AWS and how they interact with one another can be a bit daunting until you have some experience. It's very important that you don't leave services running when you aren't using them, because you will be charged for that. If you want to make sure that you haven't left anything running, the easiest thing to do is to check the 'Costs Explorer' page inside AWS. You can find this by searching for 'AWS Cost Explorer' in the search bar on the main page for the AWS Console. This service will show you what your usage is for everything that you can be charged for. It's quite intuitive and easy to use, and should make it easy to see if you are accruing charges because you left something running that you didn't realize. If you left something running that you aren't aware of, you'll see it here -- once you've noticed it, just navigate to the service in question and deactivate it. \n",
    "\n",
    "## Summary\n",
    "\n",
    "In this codealong, we learned how to productionize our own model in AWS and set up an inference endpoint!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
